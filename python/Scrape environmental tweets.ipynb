{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape tweets regarding global warming and fossil fuels from 2015 - 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tweets from 2015 - 2018 regarding global warming and fossil fuels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials, collect_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credentials\n",
    "premium_search_args = load_credentials(\"/Users/keerthanasankar/Documents/input-onlineyamltools.txt\", \n",
    "                                       account_type=\"premium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query criteria\n",
    "rule = gen_rule_payload(\"global warming\", results_per_call = 100, from_date = \"2017-06-01\", to_date = \"2017-12-31\")\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call api and collect tweets\n",
    "tweets = collect_results(rule,\n",
    "                         max_results=500,\n",
    "                         result_stream_args=premium_search_args) # change this if you need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#see results\n",
    "[print(tweet.all_text) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to file\n",
    "import json\n",
    "with open('2017-global.json', 'w') as outfile:\n",
    "    json.dump(tweets, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain location data for tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Tweets are stored in in file \"fname\". In the file used for this script, \n",
    "# each tweet was stored on one line\n",
    "\n",
    "fname = '2017-global.json'\n",
    "counter=0\n",
    "with open(fname, 'r') as f:\n",
    "    \n",
    "    #Create dictionary to later be stored as JSON. All data will be included\n",
    "    # in the list 'data'\n",
    "    users_with_geodata = {\n",
    "        \"data\": []\n",
    "    }\n",
    "    all_users = []\n",
    "    total_tweets = 0\n",
    "    geo_tweets  = 0\n",
    "    for line in f:\n",
    "        \n",
    "        while counter < 500:\n",
    "            print(counter)\n",
    "            tweet = json.loads(line)\n",
    "            #print(tweet[2])\n",
    "            if tweet[counter]['user']['id']:\n",
    "                total_tweets += 1 \n",
    "                user_id = tweet[counter]['user']['id']\n",
    "                if user_id not in all_users:\n",
    "                    all_users.append(user_id)\n",
    "\n",
    "                    #Give users some data to find them by. User_id listed separately \n",
    "                    # to make iterating this data later easier\n",
    "                    user_data = {\n",
    "                        \"user_id\" : tweet[counter]['user']['id'],\n",
    "                        \"tweet\": tweet[counter][\"text\"],\n",
    "                        \"features\" : {\n",
    "                            \"name\" : tweet[counter]['user']['name'],\n",
    "                            \"id\": tweet[counter]['user']['id'],\n",
    "                            \"screen_name\": tweet[counter]['user']['screen_name'],\n",
    "                            \"tweets\" : 1,\n",
    "                            \"location\": tweet[counter]['user']['location'],\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    #Iterate through different types of geodata to get the variable primary_geo\n",
    "                    if tweet[0]['coordinates']:\n",
    "                        user_data[\"features\"][\"primary_geo\"] = str(tweet[counter]['coordinates'][tweet[counter]['coordinates'].keys()[1]][1]) + \", \" + str(tweet[counter]['coordinates'][tweet[counter]['coordinates'].keys()[1]][0])\n",
    "                        user_data[\"features\"][\"geo_type\"] = \"Tweet coordinates\"\n",
    "                    elif tweet[0]['place']:\n",
    "                        user_data[\"features\"][\"primary_geo\"] = tweet[counter]['place']['full_name'] + \", \" + tweet[counter]['place']['country']\n",
    "                        user_data[\"features\"][\"geo_type\"] = \"Tweet place\"\n",
    "                    else:\n",
    "                        user_data[\"features\"][\"primary_geo\"] = tweet[counter]['user']['location']\n",
    "                        user_data[\"features\"][\"geo_type\"] = \"User location\"\n",
    "\n",
    "                    #Add only tweets with some geo data to .json. Comment this if you want to include all tweets.\n",
    "                    if user_data[\"features\"][\"primary_geo\"]:\n",
    "                        users_with_geodata['data'].append(user_data)\n",
    "                        geo_tweets += 1\n",
    "\n",
    "                #If user already listed, increase their tweet count\n",
    "                elif user_id in all_users:\n",
    "                    for user in users_with_geodata[\"data\"]:\n",
    "                        if user_id == user[\"user_id\"]:\n",
    "                            user[\"features\"][\"tweets\"] += 1\n",
    "            counter+=1\n",
    "    #Count the total amount of tweets for those users that had geodata            \n",
    "    for user in users_with_geodata[\"data\"]:\n",
    "        geo_tweets = geo_tweets + user[\"features\"][\"tweets\"]\n",
    "\n",
    "    #Get some aggregated numbers on the data\n",
    "    print (\"The file included \" + str(len(all_users)) + \" unique users who tweeted with or without geo data\")\n",
    "    print (\"The file included \" + str(len(users_with_geodata['data'])) + \" unique users who tweeted with geo data, including 'location'\")\n",
    "    print (\"The users with geo data tweeted \" + str(geo_tweets) + \" out of the total \" + str(total_tweets) + \" of tweets.\")\n",
    "\n",
    "# Save data to JSON file\n",
    "with open('2017-global-locations.json', 'w') as fout:\n",
    "    fout.write(json.dumps(users_with_geodata, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean location data and get state name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"viz_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert json to csv before running this code\n",
    "\n",
    "global_15 = pd.read_csv('2015-global-locations.csv')\n",
    "global_16 = pd.read_csv('2016-global-locations.csv')\n",
    "fossil_15 = pd.read_csv('2015-fossil-locations.csv')\n",
    "fossil_16 = pd.read_csv('2016-fossil-locations.csv')\n",
    "global_17 = pd.read_csv('2017-global-locations.csv')\n",
    "global_18 = pd.read_csv('2018-global-locations.csv')\n",
    "fossil_17 = pd.read_csv('2017-fossil-locations.csv')\n",
    "fossil_17 = pd.read_csv('2018-fossil-locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new column for states\n",
    "global_17['state']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the state name\n",
    "def find_state(df):\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            state = geolocator.geocode(row.features__location).raw['display_name'].split(',')\n",
    "            if (state[-1]==' USA'):\n",
    "                df.set_value(index, 'state', state[-2])\n",
    "            else:\n",
    "                df.set_value(index, 'state', '')\n",
    "        except:\n",
    "            df.set_value(index, 'state', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call function\n",
    "find_state(global_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function on all files\n",
    "find_state(global_15)\n",
    "find_state(fossil_15)\n",
    "find_state(fossil_16)\n",
    "find_state(global_16)\n",
    "find_state(fossil_17)\n",
    "find_state(fossil_18)\n",
    "find_state(global_17)\n",
    "find_state(global_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print data to file\n",
    "global_15.to_csv('global_15_tweets.csv', index=False)\n",
    "global_16.to_csv('global_16_tweets.csv', index=False)\n",
    "fossil_15.to_csv('fossil_15_tweets.csv', index=False)\n",
    "fossil_16.to_csv('fossil_16_tweets.csv', index=False)\n",
    "global_17.to_csv('global_17_tweets.csv', index=False)\n",
    "global_18.to_csv('global_18_tweets.csv', index=False)\n",
    "fossil_17.to_csv('fossil_17_tweets.csv', index=False)\n",
    "fossil_18.to_csv('fossil_18_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/searchtweets/1.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.mikaelbrunila.fi/2017/03/27/scraping-extracting-mapping-geodata-twitter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/geopy/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
